{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cacd5fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from livelossplot import PlotLosses\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from tqdm import tqdm \n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "\n",
    "from cvae_torch import CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144c0467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - try improve results\n",
    "# - save model\n",
    "# - add validation with livelossplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "719eaf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/Ferguson_fire_train.npy'\n",
    "val_path = 'data/Ferguson_fire_test.npy'\n",
    "test_path = 'data/Ferguson_fire_obs.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d7586b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_decreasing_images(images):\n",
    "    decreasing_indices = []\n",
    "    previous_ones_count = None\n",
    "    for i in range(len(images)):\n",
    "        current_image = images[i]\n",
    "        ones_count = np.count_nonzero(current_image == 1)\n",
    "        if previous_ones_count is not None and ones_count < previous_ones_count:\n",
    "            decreasing_indices.append(i)\n",
    "        previous_ones_count = ones_count\n",
    "    return decreasing_indices\n",
    "\n",
    "def create_x_y(data,indices):\n",
    "    x = np.split(data,indices)\n",
    "    y = np.split(data,indices)\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i][:-1]\n",
    "        y[i] = y[i][1:]\n",
    "    return np.concatenate(x),np.concatenate(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c980934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process data\n",
    "\n",
    "train_path = 'data/Ferguson_fire_train.npy'\n",
    "val_path = 'data/Ferguson_fire_test.npy'\n",
    "test_path = 'data/Ferguson_fire_obs.npy'\n",
    "\n",
    "train_data = np.array(np.load(open(train_path,'rb')))\n",
    "train_data_x, train_data_y = create_x_y(train_data, find_decreasing_images(train_data))\n",
    "tensor_x = torch.Tensor(train_data_x)\n",
    "tensor_y = torch.Tensor(train_data_y)\n",
    "train_dataset = TensorDataset(tensor_x,tensor_y)\n",
    "\n",
    "val_data = np.array(np.load(open(val_path,'rb')))\n",
    "val_data_x, val_data_y = create_x_y(val_data, find_decreasing_images(val_data))\n",
    "tensor_x = torch.Tensor(val_data_x)\n",
    "tensor_y = torch.Tensor(val_data_y)\n",
    "val_dataset = TensorDataset(tensor_x,tensor_y)\n",
    "\n",
    "test_data = np.array(np.load(open(test_path,'rb')))\n",
    "test_data_1D = np.reshape(test_data, (np.shape(test_data)[0],np.shape(test_data)[1]*np.shape(test_data)[2]))\n",
    "test_data_1D_shifted = torch.Tensor(test_data_1D[1:])\n",
    "test_data_1D = torch.Tensor(test_data_1D[:-1])\n",
    "test_dataset = TensorDataset(test_data_1D,test_data_1D_shifted)\n",
    "\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = data.DataLoader(dataset=val_dataset, batch_size=2*32, shuffle=False)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d973f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e621557",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Encoder_Conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Class contains the Encoder (image -> latent).\n",
    "        '''\n",
    "        super(VAE_Encoder_Conv, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 20, 5, padding=2),  # Pad so that image dims are preserved\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(2, stride=2)  # Halves the spatial dimensions\n",
    "        )  # Dims in 256x256 -> out 128x128\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(20, 40, 5, padding=2), \n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(2, stride=2) \n",
    "        )  # Dims in 128x128 -> out 64x64\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(40, 60, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )  # Dims in 64x64 -> out 32x32\n",
    "\n",
    "        self.layerMu = nn.Sequential(\n",
    "            nn.Conv2d(60, 120, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(2, stride=2) \n",
    "        )  # Dims in 32x32 -> out 16x16\n",
    "\n",
    "        self.layerSigma = nn.Sequential(\n",
    "            nn.Conv2d(60, 120, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(2, stride=2) \n",
    "        )  # Dims in 32x32 -> out 16x16\n",
    "\n",
    "    def forward(self, x, print_shape=True): \n",
    "        '''\n",
    "        x: [float] the MNIST image\n",
    "        '''\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        mu =  self.layerMu(x)\n",
    "        sigma = self.layerSigma(x)\n",
    "        return mu, sigma\n",
    "    \n",
    "class VAE_Decoder_Conv(nn.Module):  \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Class contains the Decoder (latent -> image).\n",
    "        '''\n",
    "\n",
    "        super(VAE_Decoder_Conv, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(120, 60, 4, stride=2, padding=1),  # Upsample by a factor of 2\n",
    "            nn.GELU()\n",
    "        )  # Dims in 16x16 -> out 32x32\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(60, 40, 4, stride=2, padding=1), \n",
    "            nn.GELU()\n",
    "        )  # Dims in 32x32 -> out 64x64\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(40, 20, 4, stride=2, padding=1),\n",
    "            nn.GELU()\n",
    "        )  # Dims in 64x64 -> out 128x128\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(20, 10, 4, stride=2, padding=1),\n",
    "            nn.GELU()\n",
    "        )  # Dims in 128x128 -> out 256x256\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(10, 1, 5, stride=1, padding=2),  # Preserve spatial dimensions\n",
    "            nn.Tanh()\n",
    "        )  # Dims in 256x256 -> out 256x256\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        return x\n",
    " \n",
    "    \n",
    "class VAE_Conv(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        '''\n",
    "        Class combines the Encoder and the Decoder with a VAE latent space.\n",
    "        '''\n",
    "        super(VAE_Conv, self).__init__()\n",
    "        self.device = device\n",
    "        self.encoder = VAE_Encoder_Conv()\n",
    "        self.decoder = VAE_Decoder_Conv()\n",
    "        self.distribution = torch.distributions.Normal(0, 1)  # Sample from N(0,1)\n",
    "\n",
    "    def sample_latent_space(self, mu, sigma):\n",
    "        epsilon = 1e-8  # Small epsilon value to avoid zero or negative sigma\n",
    "        sigma = torch.clamp(sigma, epsilon)  # Ensure sigma is not zero or negative\n",
    "        z = mu + sigma * self.distribution.sample(mu.shape).to(self.device)  # Sample the latent distribution\n",
    "        kl_div = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()  # A term, which is required for regularisation\n",
    "        return z, kl_div\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x - [float] A batch of images from the data-loader\n",
    "        '''\n",
    "        mu, sigma = self.encoder(x)  # Run the image through the Encoder\n",
    "        z, kl_div = self.sample_latent_space(mu, sigma)  # Take the output of the encoder and get the latent vector \n",
    "        z = self.decoder(z)  # Return the output of the decoder (the predicted image)\n",
    "        return z, kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e515f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1\n",
      "Train:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|███████████████████████                  | 218/388 [07:54<06:07,  2.16s/it]"
     ]
    }
   ],
   "source": [
    "def train(autoencoder, train_data, val_data, kl_div_on=True, epochs=10, device='cpu', patience=3):\n",
    "    opt = torch.optim.Adam(autoencoder.parameters())\n",
    "    liveloss = PlotLosses()    \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch+1} of {epochs}')\n",
    "        logs = {}\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        # Training\n",
    "        autoencoder.train()\n",
    "        print('Train:')\n",
    "        for batch, label in tqdm(train_data):\n",
    "            batch = batch.to(device)\n",
    "            batch = batch.reshape(batch.shape[0], 1, batch.shape[1], batch.shape[2])\n",
    "            opt.zero_grad()\n",
    "            x_hat, KL = autoencoder(batch)\n",
    "            loss = ((batch - x_hat) ** 2).sum() + KL\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_data)\n",
    "        logs['loss'] = train_loss\n",
    "        # Validation\n",
    "        autoencoder.eval()\n",
    "        print('Val:')\n",
    "        with torch.no_grad():\n",
    "            for batch, label in tqdm(val_data):\n",
    "                batch = batch.to(device)\n",
    "                batch = batch.reshape(batch.shape[0], 1, batch.shape[1], batch.shape[2])\n",
    "                x_hat, KL = autoencoder(batch)\n",
    "                loss = ((batch - x_hat) ** 2).sum() + KL\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_data)\n",
    "        logs['val_loss'] = val_loss\n",
    "        liveloss.update(logs)\n",
    "        liveloss.send()\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping: No improvement in validation loss for {patience} epochs.\")\n",
    "                break\n",
    "    return autoencoder\n",
    "\n",
    "device = 'cpu'\n",
    "vae = VAE_Conv(device).to(device)\n",
    "vae = train(vae, train_loader, val_loader, epochs=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41767a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(autoencoder, data, kl_div_on=True, epochs=10, device='cpu'):\n",
    "#     opt = torch.optim.Adam(autoencoder.parameters())\n",
    "#     for epoch in range(epochs):  # Run data over numerous epochs\n",
    "#         print(f'Epoch {epoch+1} of {epochs}')\n",
    "#         for batch, label in tqdm(data):  # Iterate over the batches of images and labels\n",
    "#             batch = batch.reshape(batch.shape[0],1,batch.shape[1],batch.shape[2]).to(device)  # Send batch of images to the GPU            \n",
    "#             opt.zero_grad()  # Set optimiser grad to 0\n",
    "#             x_hat, KL = autoencoder(batch)  # Generate predicted images (x_hat) by running batch of images through autoencoder\n",
    "#             loss = ((batch - x_hat)**2).sum() + KL  # Calculate combined loss\n",
    "#             loss.backward()  # Back-propagate\n",
    "#             opt.step()  # Step the optimiser\n",
    "#     return autoencoder  # Return the trained autoencoder (for later analysis)\n",
    "\n",
    "# device = 'cpu'\n",
    "\n",
    "# vae_conv = VAE_Conv(device).to(device)\n",
    "# vae_conv = train(vae_conv.train(True), train_loader, epochs=1, device=device)\n",
    "\n",
    "# print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525b47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, labels = next(iter(train_loader))  # Get the first batch of images\n",
    "# # images, labels = next(iter(test_loader))  # Get the first batch of images\n",
    "# print(images.shape)  # Get the first image from the batch\n",
    "# vae_conv1 = vae_conv.eval()\n",
    "\n",
    "# _, ax = plt.subplots(2, 5, figsize=[18.5, 6])\n",
    "# for n, idx  in enumerate(torch.randint(0,images.shape[0], (5,))):\n",
    "#     recon, _ = vae_conv(images[idx].unsqueeze(0).cuda())  # Are mu and sigma correct\n",
    "#     ax[0, n].imshow(images[idx].squeeze())\n",
    "#     ax[1, n].imshow(recon.cpu().detach().squeeze())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
