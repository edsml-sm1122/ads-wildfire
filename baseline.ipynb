{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45907ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from livelossplot import PlotLosses\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from tqdm import tqdm \n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "\n",
    "from cvae_torch import CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d492e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/Ferguson_fire_train.npy'\n",
    "val_path = 'data/Ferguson_fire_test.npy'\n",
    "test_path = 'data/Ferguson_fire_obs.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd92fe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_decreasing_images(images):\n",
    "    decreasing_indices = []\n",
    "    previous_ones_count = None\n",
    "    for i in range(len(images)):\n",
    "        current_image = images[i]\n",
    "        ones_count = np.count_nonzero(current_image == 1)\n",
    "        if previous_ones_count is not None and ones_count < previous_ones_count:\n",
    "            decreasing_indices.append(i)\n",
    "        previous_ones_count = ones_count\n",
    "    return decreasing_indices\n",
    "\n",
    "def create_x_y(data,indices):\n",
    "    x = np.split(data,indices)\n",
    "    y = np.split(data,indices)\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i][:-1]\n",
    "        y[i] = y[i][1:]\n",
    "    return np.concatenate(x),np.concatenate(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bbeec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process data\n",
    "\n",
    "train_path = 'data/Ferguson_fire_train.npy'\n",
    "val_path = 'data/Ferguson_fire_test.npy'\n",
    "test_path = 'data/Ferguson_fire_obs.npy'\n",
    "\n",
    "train_data = np.array(np.load(open(train_path,'rb')))\n",
    "train_data_x, train_data_y = create_x_y(train_data, find_decreasing_images(train_data))\n",
    "tensor_x = torch.Tensor(train_data_x)\n",
    "tensor_y = torch.Tensor(train_data_y)\n",
    "train_dataset = TensorDataset(tensor_x,tensor_y)\n",
    "\n",
    "val_data = np.array(np.load(open(val_path,'rb')))\n",
    "val_data_x, val_data_y = create_x_y(val_data, find_decreasing_images(val_data))\n",
    "tensor_x = torch.Tensor(val_data_x)\n",
    "tensor_y = torch.Tensor(val_data_y)\n",
    "val_dataset = TensorDataset(tensor_x,tensor_y)\n",
    "\n",
    "# test_data = np.array(np.load(open(test_path,'rb')))\n",
    "# test_data_1D = np.reshape(test_data, (np.shape(test_data)[0],np.shape(test_data)[1]*np.shape(test_data)[2]))\n",
    "# test_data_1D_shifted = test_data_1D[1:]\n",
    "# test_data_1D = test_data_1D[:-1]\n",
    "\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = data.DataLoader(dataset=val_dataset, batch_size=2*128, shuffle=False)\n",
    "# test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d2fa815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9d1ea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Encoder_Conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Class contains the Encoder (image -> latent).\n",
    "        '''\n",
    "        super(VAE_Encoder_Conv, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 20, 5, padding=2),  # Pad so that image dims are preserved\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(2, stride=2)  # Halves the spatial dimensions\n",
    "        )  # Dims in 256x256 -> out 128x128\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(20, 40, 5, padding=2), \n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(2, stride=2) \n",
    "        )  # Dims in 128x128 -> out 64x64\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(40, 60, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )  # Dims in 64x64 -> out 32x32\n",
    "\n",
    "        self.layerMu = nn.Sequential(\n",
    "            nn.Conv2d(60, 120, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(2, stride=2) \n",
    "        )  # Dims in 32x32 -> out 16x16\n",
    "\n",
    "        self.layerSigma = nn.Sequential(\n",
    "            nn.Conv2d(60, 120, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(2, stride=2) \n",
    "        )  # Dims in 32x32 -> out 16x16\n",
    "\n",
    "    def forward(self, x, print_shape=True): \n",
    "        '''\n",
    "        x: [float] the MNIST image\n",
    "        '''\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        mu =  self.layerMu(x)\n",
    "        sigma = self.layerSigma(x)\n",
    "        return mu, sigma\n",
    "    \n",
    "class VAE_Decoder_Conv(nn.Module):  \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Class contains the Decoder (latent -> image).\n",
    "        '''\n",
    "\n",
    "        super(VAE_Decoder_Conv, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(120, 60, 4, stride=2, padding=1),  # Upsample by a factor of 2\n",
    "            nn.GELU()\n",
    "        )  # Dims in 16x16 -> out 32x32\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(60, 40, 4, stride=2, padding=1), \n",
    "            nn.GELU()\n",
    "        )  # Dims in 32x32 -> out 64x64\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(40, 20, 4, stride=2, padding=1),\n",
    "            nn.GELU()\n",
    "        )  # Dims in 64x64 -> out 128x128\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(20, 10, 4, stride=2, padding=1),\n",
    "            nn.GELU()\n",
    "        )  # Dims in 128x128 -> out 256x256\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(10, 1, 5, stride=1, padding=2),  # Preserve spatial dimensions\n",
    "            nn.Tanh()\n",
    "        )  # Dims in 256x256 -> out 256x256\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        return x\n",
    " \n",
    "    \n",
    "class VAE_Conv(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        '''\n",
    "        Class combines the Encoder and the Decoder with a VAE latent space.\n",
    "        '''\n",
    "        super(VAE_Conv, self).__init__()\n",
    "        self.device = device\n",
    "        self.encoder = VAE_Encoder_Conv()\n",
    "        self.decoder = VAE_Decoder_Conv()\n",
    "        self.distribution = torch.distributions.Normal(0, 1)  # Sample from N(0,1)\n",
    "\n",
    "    def sample_latent_space(self, mu, sigma):\n",
    "        z = mu + sigma * self.distribution.sample(mu.shape).to(self.device)  # Sample the latent distribution\n",
    "        kl_div = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()  # A term, which is required for regularisation\n",
    "        return z, kl_div\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x - [float] A batch of images from the data-loader\n",
    "        '''\n",
    "        mu, sigma = self.encoder(x)  # Run the image through the Encoder\n",
    "        z, kl_div = self.sample_latent_space(mu, sigma)  # Take the output of the encoder and get the latent vector \n",
    "        z = self.decoder(z)  # Return the output of the decoder (the predicted image)\n",
    "        return z, kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b173126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|██████████████████▏                        | 41/97 [05:23<07:34,  8.12s/it]"
     ]
    }
   ],
   "source": [
    "def train(autoencoder, train_data, val_data, kl_div_on=True, epochs=10, device='cpu', patience=3):\n",
    "    opt = torch.optim.Adam(autoencoder.parameters())\n",
    "    liveloss = PlotLosses()    \n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        logs = {}\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        # Training\n",
    "        autoencoder.train()\n",
    "        for batch, label in tqdm(train_data):\n",
    "            batch = batch.to(device)\n",
    "            batch = batch.reshape(batch.shape[0], 1, batch.shape[1], batch.shape[2])\n",
    "            opt.zero_grad()\n",
    "            x_hat, KL = autoencoder(batch)\n",
    "            loss = ((batch - x_hat) ** 2).sum() + KL\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_data)\n",
    "        logs['loss'] = train_loss\n",
    "        # Validation\n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch, label in tqdm(val_data):\n",
    "                batch = batch.to(device)\n",
    "                batch = batch.reshape(batch.shape[0], 1, batch.shape[1], batch.shape[2])\n",
    "                x_hat, KL = autoencoder(batch)\n",
    "                loss = ((batch - x_hat) ** 2).sum() + KL\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_data)\n",
    "        logs['val_loss'] = val_loss\n",
    "        liveloss.update(logs)\n",
    "        liveloss.send()\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping: No improvement in validation loss for {patience} epochs.\")\n",
    "                break\n",
    "    return autoencoder\n",
    "\n",
    "device = 'cpu'\n",
    "vae = VAE_Conv(device).to(device)\n",
    "vae = train(vae, train_loader, val_loader, epochs=10, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
